
### generate data
#' @param n_views number of views
#' @param n_features number of features in each view 
#' @param n_samples number of samples in each group
#' @param n_groups number of groups
#' @param n_factors number of factors
#' @param likelihood likelihood for each view, one of "gaussian" (default), "bernoulli", "poisson",
#'  or a character vector of length n_views
#' @param lscales vector of lengthscales, needs to be of length n_factors (default is 0 - no smooth factors)
#' @param sample_cov (only for use with MEFISTO) matrix of sample covariates for one group with covariates in rows and samples in columns 
#' or "equidistant" for sequential ordering, default is NULL (no smooth factors)
#' @param as.data.frame return data and covariates as long dataframe 
#' @return Returns a list containing the simulated data and simulation parameters.
#' @importFrom stats rnorm rbinom rpois
#' @importFrom dplyr left_join
#' @importFrom stats dist
#' @export
#' @examples
#' # Generate a simulated data set
#' MOFAexample <- make_example_data()


data_gen_omics <- function(n_views=3, n_features=100, n_samples = 50, n_groups = 1,
                              n_factors = 5, likelihood = "gaussian",
                              lscales = 1, sample_cov = NULL, as.data.frame = FALSE) {
  
  # Sanity checks
  if (!all(likelihood %in% c("gaussian", "bernoulli", "poisson")))
    stop("Likelihood not implemented: Use either gaussian, bernoulli or poisson")
  
  if(length(lscales) == 1)
    lscales = rep(lscales, n_factors)
  if(!length(lscales) == n_factors)
    stop("Lengthscalces lscales need to be of length n_factors")
  if(all(lscales == 0)){
    sample_cov <- NULL
  }
  
  if (length(likelihood)==1) likelihood <- rep(likelihood, n_views) 
  if (!length(likelihood) == n_views) 
    stop("Likelihood needs to be a single string or matching the number of views!")
  
  if(!is.null(sample_cov)){
    if(sample_cov[1] == "equidistant") {
      sample_cov <- seq_len(n_samples)
    }
    if(is.null(dim(sample_cov))) sample_cov <- matrix(sample_cov, nrow = 1)
    if(ncol(sample_cov) != n_samples){
      stop("Number of columns in sample_cov must match number of samples n_samples.")
    }
    
    # Simulate covariance for factors
    Sigma = lapply(lscales, function(ls) {
      if(ls == 0) diag(1, n_samples)
      else (1) * exp(-as.matrix(stats::dist(t(sample_cov)))^2/(2*ls^2))
      # else (1-0.001) * exp(-as.matrix(stats::dist(t(sample_cov)))^2/(2*ls^2)) + diag(0.001, n_samples)
    })
    
    # simulate factors
    alpha_z <- NULL
    S_z <- lapply(seq_len(n_groups), function(vw) matrix(1, nrow=n_samples, ncol=n_factors))
    Z <-  vapply(seq_len(n_factors), function(fc) mvtnorm::rmvnorm(1, rep(0, n_samples), Sigma[[fc]]), numeric(n_samples))
    colnames(Z) <- paste0("simulated_factor_", 1:ncol(Z))
    Z <- lapply(seq_len(n_groups), function(gr) Z)
    sample_cov <- Reduce(cbind, lapply(seq_len(n_groups), function(gr) sample_cov))
  } else {
    # set sparsity for factors
    theta_z <- 0.5
    
    # set ARD prior for factors, each factor being active in at least one group
    alpha_z <- vapply(seq_len(n_factors), function(fc) {
      active_gw <- sample(seq_len(n_groups), 1)
      alpha_fc <- sample(c(1, 1000), n_groups, replace = TRUE)
      if(all(alpha_fc==1000)) alpha_fc[active_gw] <- 1
      alpha_fc
    }, numeric(n_groups))
    alpha_z <- matrix(alpha_z, nrow=n_factors, ncol=n_groups, byrow=TRUE)
    
    # simulate facors 
    S_z <- lapply(seq_len(n_groups), function(vw) matrix(rbinom(n_samples * n_factors, 1, theta_z),
                                                         nrow=n_samples, ncol=n_factors))
    Z <- lapply(seq_len(n_groups), function(vw) vapply(seq_len(n_factors), function(fc) rnorm(n_samples, 0, sqrt(1/alpha_z[fc,vw])), numeric(n_samples)))
  }
  
  # set sparsity for weights
  theta_w <- 0.5
  
  # set ARD prior, each factor being active in at least one view
  alpha_w <- vapply(seq_len(n_factors), function(fc) {
    active_vw <- sample(seq_len(n_views), 1)
    alpha_fc <- sample(c(1, 1000), n_views, replace = TRUE)
    if(all(alpha_fc==1000)) alpha_fc[active_vw] <- 1
    alpha_fc
  }, numeric(n_views))
  alpha_w <- matrix(alpha_w, nrow=n_factors, ncol=n_views, byrow=TRUE)
  
  # simulate weights 
  S_w <- lapply(seq_len(n_views), function(vw) matrix(rbinom(n_features*n_factors, 1, theta_w),
                                                      nrow=n_features, ncol=n_factors))
  W <- lapply(seq_len(n_views), function(vw) vapply(seq_len(n_factors), function(fc) rnorm(n_features, 0, sqrt(1/alpha_w[fc,vw])), numeric(n_features)))
  
  # set noise level (for gaussian likelihood)
  tau <- 10
  
  # pre-compute linear term and rbind groups
  mu <- lapply(seq_len(n_views), function(vw) lapply(seq_len(n_groups), function(gw)  (S_z[[gw]]*Z[[gw]]) %*% t(S_w[[vw]]*W[[vw]])))
  mu <- lapply(mu, function(l) Reduce(rbind, l))
  groups <- rep(paste("group",seq_len(n_groups), sep = "_"), each = n_samples)
  
  # simulate data according to the likelihood
  data <- lapply(seq_len(n_views), function(vw){
    lk <- likelihood[vw]
    if (lk == "gaussian"){
      dd <- t(mu[[vw]] + rnorm(length(mu[[vw]]),0,sqrt(1/tau)))
    }
    else if (lk == "poisson"){
      term <- log(1+exp(mu[[vw]]))
      dd <- t(apply(term, 2, function(tt) rpois(length(tt),tt)))
    }
    else if (lk == "bernoulli") {
      term <- 1/(1+exp(-mu[[vw]]))
      dd <- t(apply(term, 2, function(tt) rbinom(length(tt),1,tt)))
    }
    colnames(dd) <- paste0("sample_", seq_len(ncol(dd)))
    rownames(dd) <- paste0("feature_", seq_len(nrow(dd)),"_view", vw)
    dd
  })
  
  if(!is.null(sample_cov)) {
    colnames(sample_cov) <- colnames(data[[1]])
    rownames(sample_cov) <- paste0("covariate_", seq_len(nrow(sample_cov)))
  }
  
  names(data) <- paste0("view_", seq_len(n_views))
  
  if(as.data.frame){
    gr_df <- data.frame(group = groups, sample = colnames(data[[1]]))
    dat <- lapply(names(data), function(vw){
      tmp <- data[[vw]]
      df <- melt(tmp, varnames = c("feature", "sample"))
      df$view <- vw
      df
    })
    data <- bind_rows(dat)
    data <- dplyr::left_join(data, gr_df, by = "sample")
    
    sample_cov <- melt(sample_cov, varnames = c("covariate", "sample"))
  }
  return(list(data = data, groups = groups, alpha_w=alpha_w, alpha_z =alpha_z,
              lscales = lscales, sample_cov = sample_cov, Z = Z))
}

######################################
##########################
## data simulation function ##
## N: number of subjects; seed: random seed ##
data_gen_ls <- function(seed, tp, N,Z_hat){
  set.seed(seed)
  
  eps <- 1e-5
  G <- 15  ## quadrature points
  w <- gauss.quad(G, 'legendre')$weights
  node <- gauss.quad(G, 'legendre')$nodes
  
  ##to get parameter setting from tp
  J <- tp$J
  tnew <- c(0:100)/100
  mu <- list(
    f1 = function(t) 10 + 5*t + 5*t^2,
    f2 = function(t) 10 + sqrt(t), #10 + 10*sqrt(t) + 5*sin(2*pi*t),
    f3 = function(t) sin(2*pi*t) + cos(2*pi*t) + log(t+1)
  )
  
  mu_tnew <- list(mu1 = mu[[1]](tnew), mu2 = mu[[2]](tnew), mu3 = mu[[3]](tnew))
  
  L0 <- 2
  phi <- list(
    f1 = function(t) sqrt(2)*sin(pi*t),
    f2 = function(t) sqrt(2)*cos(pi*t)
  )
  
  L1 <- 1
  psi <- list(
    f1 = function(t) sqrt(2)*cos(2*pi*t)
  )
  
  d0 <- tp$d0
  d1 <- tp$d1
  
  beta <- tp$beta
  omega <- tp$omega
  
  logh0 <- tp$logh0
  gamma_x <- tp$gamma_x
  gamma_z <- tp$gamma_z ##diff
  gamma0 <- tp$gamma0 ##
  gamma11 <- tp$gamma11
  gamma12 <- tp$gamma12
  gamma13 <- tp$gamma13
  
  xi <- matrix(NA, N, L0)
  zeta1 <- zeta2 <- zeta3 <- matrix(NA, N, L1)
  
  for (l in 1:L0){
    xi[, l] <- (sqrt(d0[l])*scale(rnorm(N)))[, 1]
  }
  
  for (l in 1:L1){
    zeta1[, l] <- (sqrt(d1[l])*scale(rnorm(N)))[, 1]
    zeta2[, l] <- (sqrt(d1[l])*scale(rnorm(N)))[, 1]
    zeta3[, l] <- (sqrt(d1[l])*scale(rnorm(N)))[, 1]
  }
  zeta <- list(zeta1, zeta2, zeta3)
  ## to generate mu, psi and phi
  ID <- time <- time.points <- NULL
  mu_obs <- vector('list', J)
  phi_obs <- vector('list', L0)
  psi_obs <- vector('list', L1)
  for (i in 1:N){
    tmp.obstime.2 <- sample((1:10)/100, size = 1)
    tmp.obstime.2 <- tmp.obstime.2 + (0:10)/10
    if (tmp.obstime.2[11]>max(tnew)) tmp.obstime.2 <- tmp.obstime.2[-11]
    tmp.obstime.2 <- c(0, tmp.obstime.2) ##sparse observed time
    
    time <- c(time, tmp.obstime.2)
    time.points <- c(time.points, length(tmp.obstime.2))
    ID <- c(ID, rep(i, length(tmp.obstime.2)))
    
    for (j in 1:J) mu_obs[[j]] <- c(mu_obs[[j]], mu_tnew[[j]][tmp.obstime.2*100+1])
    
    for (l in 1:L0) phi_obs[[l]] <- c(phi_obs[[l]], phi[[l]](tmp.obstime.2))
    for (l in 1:L1) psi_obs[[l]] <- c(psi_obs[[l]], psi[[l]](tmp.obstime.2))
  }
  
  s.time.points <- sum(time.points) ##total observed time points
  phi_obs_mat <- matrix(NA, s.time.points, L0)
  psi_obs_mat <- matrix(NA, s.time.points, L1)
  for (l in 1:L0) phi_obs_mat[, l] <- phi_obs[[l]]
  for (l in 1:L1) psi_obs_mat[, l] <- psi_obs[[l]]
  
  ### to generate Y
  Y <- true.Y <- err <- vector('list', J)
  for (j in 1:J) err[[j]] <- rsn(s.time.points, xi = 0, omega = omega[j], alpha = 0)
  
  for (i in 1:s.time.points){
    for (j in 1:J){
      tmp.Y <- mu_obs[[j]][i] + beta[j]*(sum(xi[ID[i], ]*phi_obs_mat[i, ]) + 
                                           sum(zeta[[j]][ID[i], ]*psi_obs_mat[i, ]))
      true.Y[[j]] <- c(true.Y[[j]], tmp.Y)
      Y[[j]] <- c(Y[[j]], tmp.Y + err[[j]][i])
    }
  }
  
  long.dat <- data.frame(ID = ID, time = time, 
                         y1 = Y[[1]], y2 = Y[[2]], y3 = Y[[3]])
  # browser()
  ###to generate survival data
  C <- runif(N, 0, 1.6)   ## censoring time
  C <- pmin(C, rep(max(tnew), N))
  # browser()
  surv.dat <- data.frame(ID = rep(1:N), 
                         x = rbinom(N, 2, 0.4), 
                         z_hat = list(Z_hat),
                         surv_time = rep(NA, N),
                         status = rep(NA, N))
  ## to compute event time
  H <- function(t){
    f <- function(t) exp(gamma0*(xi[i, 1]*sqrt(2)*sin(pi*t) + xi[i, 2]*sqrt(2)*cos(pi*t)) + 
                           gamma11*zeta1[i, 1]*sqrt(2)*cos(2*pi*t) + 
                           gamma12*zeta2[i, 1]*sqrt(2)*cos(2*pi*t) + 
                           gamma13*zeta3[i, 1]*sqrt(2)*cos(2*pi*t))
    sum.approx <- sum(w*f(t/2*node + t/2))
    return(exp(logh0 + 
                 surv.dat$x[i]*gamma_x +
                 as.matrix(surv.dat[,3:(3+n_factors-1)],ncol=n_factors,byrow=T)[i,]%*% gamma_z ##diff
               )*t/2*sum.approx)
  }
  
  for (i in 1:N){
    Ht <- NULL
    for (t in tnew) Ht <- c(Ht, H(t))
    
    flag <- FALSE
    t0 <- 0; t1 <- 1; ## lower and upper bound for root
    ti <- Inf;
    Si <- runif(1, 0, 1) ## survival probability
    
    while (t1-t0>eps){
      root_seq <- seq(t0, t1, by = (t1-t0)/10)
      for (j in 1:10){
        Hi_left <- H(root_seq[j]) + log(Si)
        Hi_right <- H(root_seq[j+1]) + log(Si)
        ## whether we find a solution falls into (root_seq[j], root_seq[j+1])
        if (Hi_left*Hi_right<0){  
          t0 = root_seq[j]; t1 = root_seq[j+1]; flag <- TRUE
          ti <- (t1+t0)/2; break ## set ti as mid point as (t1 and t0)
        }
      }
      ## no solution found in interval (0, 1)
      if (flag==FALSE) break;
    }
    
    surv.dat$surv_time[i] <- min(ti, C[i]) ## observed survival time
    surv.dat$status[i] <- as.integer(ti<C[i])
  }
  
  long.dat2 <- long.dat[0, ]
  for (i in 1:N){
    tmp.long.dat <- long.dat[which(long.dat$ID==i), ]
    tmp.surv.time <- surv.dat$surv_time[i]
    index <- which(tmp.long.dat$time<=tmp.surv.time)
    long.dat2 <- rbind(long.dat2, tmp.long.dat[index, ])
  }
  
  ## Generate missingness ##
  inv_logit <- function(x) return(1/(1+exp(-x)))
  long.dat3 <- long.dat2
  for (i in 1:N){
    p_missing <- inv_logit(0.5*(long.dat2$time[i]-5))
    I_missing <- rbinom(3, 1, p_missing)
    if (I_missing[1]==1) long.dat3$y1[i] <- NA
    if (I_missing[2]==1) long.dat3$y2[i] <- NA
    if (I_missing[3]==1) long.dat3$y3[i] <- NA
  }
  
  sim.dat <- list(long = long.dat3, surv = surv.dat, 
                  phi_obs_mat = phi_obs_mat, psi_obs_mat = psi_obs_mat, 
                  xi = xi, zeta = zeta)
  return(sim.dat)
}

